{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redes neuronales para análisis de sentimiento sobre IMBD\n",
    "\n",
    "En este cuaderno entrenaremos y evaluaremos un modelo sencillo basado en la arquitectura Transforer para clasificar reviews de películas en positivas o negativas. Para hacerlo, además de utilizar pytorch, utilizaremos el paquete adicional `torchtext` , que nos ofrece algunas funciones útiles para trabajar con texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext.legacy import data\n",
    "from torchtext.legacy import datasets\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descargando y procesando los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En primer lugar debemos tener instalado `spacy`, con el tokenizador ingés estándar (`python -m spacy download en`), para separar las reviews en palabras.\n",
    "\n",
    "Ahora, definiremos dos variables, una el `TEXT`, que será nuestra $x$, y la otra `LABEL`, que será nuestra $y$. Fijaos en que en el caso de las features tenemos que especificar el tokenizador que vamos a utilizar, mientras que para la label simplemente será un valor numérico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(tokenize = 'spacy')\n",
    "LABEL = data.LabelField(dtype = torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, dividiremos los datos en un conjunto de entrenamiento y otro de evaluación. Fijémonos en que, al igual que como en el MNIST, torchtext ofrece un submódulo datasets que contiene los datasets de NLP más populares, como el IMDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading aclImdb_v1.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aclImdb_v1.tar.gz: 100%|██████████| 84.1M/84.1M [03:18<00:00, 425kB/s] \n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Echemos un vistazo al primer ejemplo de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['For', 'a', 'movie', 'that', 'gets', 'no', 'respect', 'there', 'sure', 'are', 'a', 'lot', 'of', 'memorable', 'quotes', 'listed', 'for', 'this', 'gem', '.', 'Imagine', 'a', 'movie', 'where', 'Joe', 'Piscopo', 'is', 'actually', 'funny', '!', 'Maureen', 'Stapleton', 'is', 'a', 'scene', 'stealer', '.', 'The', 'Moroni', 'character', 'is', 'an', 'absolute', 'scream', '.', 'Watch', 'for', 'Alan', '\"', 'The', 'Skipper', '\"', 'Hale', 'jr', '.', 'as', 'a', 'police', 'Sgt', '.'], 'label': 'pos'}\n"
     ]
    }
   ],
   "source": [
    "print(vars(train_data.examples[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como esperamos, la review ha sido clasificada como positiva `pos`.\n",
    "\n",
    "Ahora, haremos otra partición para validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1230245\n",
    "train_data, valid_data = train_data.split(random_state = random.seed(SEED))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construyendo el vocabulario\n",
    "\n",
    "Ahora que ya tenemos para la $x$ una lista de palabras, ¿cómo podemos convertirla en un vector?\n",
    "\n",
    "Now we have for $x$ a list of words, but how we can convert that to a vector?\n",
    "\n",
    "El primer paso es definir un vocabulario, esto es, un subconjunto de todas las palabras que aparecen en el dataset, y solo nos fijaremos en esas palabras. Nuestro vocabulario tendrá un tamaño de 5000 palabras. Hay muchas formas de construirlo, pero la más usual es la de que quedarnos con las 5000 palabras más comunes en nuestro dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VOCAB_SIZE = 5000\n",
    "\n",
    "TEXT.build_vocab(train_data, max_size = MAX_VOCAB_SIZE)\n",
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto es, este vocabulario recién creado, a cada palabra $w_i$ le asigna un entero distinto. El vocabulario no es más que una aplicación $V : W \\rightarrow \\lbrace 0, \\ldots, 5002 \\rbrace \\subset \\mathbb{N}$ del conjunto de palabras $W$ a los respectivos números enteros.\n",
    "\n",
    "La inclusión de dos número enteros adicionales (por eso es 5002 y no 5000) es debida a que necesitamos un código especial para las palabras \"raras\" que no están entre las 5000 más comunes, y otro número especial para indicar el final de la frase (usarlo como \"padding\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploración de features\n",
    "\n",
    "Ahora, podemos acceder al atributo `TEXT.vocab` para explorar el dataset. Por ejemplo, podemos encontrar los 20 tokens más comunes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 202447), (',', 191723), ('.', 165063), ('and', 109436), ('a', 109117), ('of', 100549), ('to', 93682), ('is', 75862), ('in', 61082), ('I', 54382), ('it', 53522), ('that', 49486), ('\"', 44094), (\"'s\", 43020), ('this', 42360), ('-', 37132), ('/><br', 35645), ('was', 35168), ('as', 30387), ('with', 29888)]\n"
     ]
    }
   ],
   "source": [
    "print(TEXT.vocab.freqs.most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por ejemplo, podemos ver que `the`es la palabra más común, apareciendo 203504 veces en nuestro corpus.\n",
    "\n",
    "El método stoi calcula la función $V$ anterior ('string to integer')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.vocab.stoi['the']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También tenemos acceso a la función inversa, de enteros a strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unk>', '<pad>', 'the', ',', '.', 'and', 'a', 'of', 'to', 'is']\n"
     ]
    }
   ],
   "source": [
    "print(TEXT.vocab.itos[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(None, {'neg': 0, 'pos': 1})\n"
     ]
    }
   ],
   "source": [
    "print(LABEL.vocab.stoi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definición del modelo: capas de Transformers\n",
    "\n",
    "Ahora que ya tenemos el vocabulario, podemos representar cada palabra como un vector one-hot, sobre el espacio $\\lbrace 0, 1 \\rbrace^{5002}$. Este espacio es muy disperso (\"vacío\"), y hace que cada palabra se encuentre a la misma distancia que cualquier otra. Así que lo primero que podemos hacer en NLP profundo es aplicar una proyección lineal a un espacio de dimensionalidad mucho más baja. Esto es, para la iésima palabra, haremos $h_i = W x_i$, donde $W$ es una matriz de tamaño $100 \\times 5002$. El 100 se refiere a la dimensión del \"embedding\". Para más información, podéis consultar por *word embeddings*.\n",
    "\n",
    "Después, tras haber calculado esta nueva representación de las palabras, podemos aplicar una capa de Transformer, una arquitectura que está diseñada para captar patrones en secuencias de símbolos, con el objetivo de aprender qué partes de una frase son más informativas respecto a su sentimiento. La arquitectura Transformer es muy reciente (https://arxiv.org/abs/1706.03762) y, al contrario que las antecesoras redes recurrentes, es más fácil de paralelizar por lo que su entrenamiento es mucho más rápido.\n",
    "\n",
    "Para ver los detalles de la arquitectura Transformer podéis consultar los siguientes artículos:\n",
    "\n",
    "* https://jalammar.github.io/illustrated-transformer/\n",
    "\n",
    "* https://nlp.seas.harvard.edu/2018/04/03/attention.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, \n",
    "                    dropout, pad_idx):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
    "        self.enc = nn.TransformerEncoderLayer(embedding_dim, 4, hidden_dim)\n",
    "        self.fc = nn.Linear(embedding_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        hidden = self.enc(embedded)\n",
    "        hidden = hidden.mean(dim=0)\n",
    "        return self.fc(hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como en el cuaderno anterior, definimos ahora los hiperparámetros y los iteradores sobre el dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size = BATCH_SIZE,\n",
    "    device = device)\n",
    "\n",
    "# Las dos dimensiones de la matriz de embeddings W:\n",
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "#  Como solo queremos predecir positivo o negativo, simplemente el output es un número escalar\n",
    "# en R^1 (usando la sigmoide para obtener una probabilidad)\n",
    "OUTPUT_DIM = 1\n",
    "# Apilaremos dos capas de Transformers\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "# Especificamos algo de dropout para regularizar la red neuronal\n",
    "DROPOUT = 0.5\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "model = Transformer(INPUT_DIM, \n",
    "            16, \n",
    "            EMBEDDING_DIM // 10, \n",
    "            OUTPUT_DIM, \n",
    "            N_LAYERS, \n",
    "            BIDIRECTIONAL, \n",
    "            DROPOUT, \n",
    "            PAD_IDX)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenando el modelo\n",
    "\n",
    "El resto del cuaderno es como el anterior, solo tenemos que definir una función auxiliar para calcular la tasa de acierto y las funciones de entrenamiento y evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float()\n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for (text, cls) in iterator:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(text).squeeze(1)\n",
    "        loss = criterion(predictions, cls)\n",
    "        acc = binary_accuracy(predictions, cls)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for (text, cls) in iterator:\n",
    "\n",
    "            predictions = model(text).squeeze(1)\n",
    "            loss = criterion(predictions, cls)\n",
    "            acc = binary_accuracy(predictions, cls)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tras definir estos bucles, entrenamos el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 5\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    torch.save(model.state_dict(), 'model.pt' + str(epoch))\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02}')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.310 | Test Acc: 87.24%\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('model.pt4'))\n",
    "\n",
    "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio** ¿Podemos hacerlo mejor? Trata de modificar los hiperparámetros para ver como cambia la tasa de acierto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio** Sustituye la capa de Transformer por una lineal y observa cómo cambia la tasa de acierto.\n",
    "\n",
    "(Puede que tengas que cambiar algo tras hacer la operacion del embedding h = Wx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
